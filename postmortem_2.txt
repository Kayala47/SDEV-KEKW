Team: Discord Dragon

Members: Kevin Ayala, Swamik Lamichhane, Max Mingst, Dana Teves

Project:
    Post Mortem: https://github.com/Kayala47/SDEV-KEKW/blob/master/post_mortem2.txt
    Primary author(s): Kevin Ayala, Swamik Lamichhane, Dana Teves
    Minutes: https://github.com/Kayala47/SDEV-KEKW/blob/master/minutes_2.txt
    Planning: https://github.com/Kayala47/SDEV-KEKW/blob/master/management_2a

Slip Days: 0

CREATION OF THE INITIAL ARCHITECTURE
Our team worked on the initial architecture for the beginning of this development cycle, and then 
refined our approach after a formal design review and team discussion and research. Our preliminary 
architecture was focused on outlining the structure of our bot as well as early prototyping for 
critical modules including dice rolling and interaction with the Python API provided by Discord. 
Upon reflection, our preliminary architecture had strong foundations, but we failed to deliver 
certain implementation details which we then reworked. As for things we believe we did well, we 
allocated individual team members separate major architecture components so that we were able to 
work and develop simultaneously, and later receive necessary feedback from other teammates that 
had “fresh eyes” in the sense that they were not a part of the initial drafting. We also worked 
quickly on developing important prototypes, and our results demonstrated that our bot was both 
feasible to implement and that we already had examples of bots using similar toolsets. These 
prototypes helped us work out details that we had glossed over in our early planning. After 
receiving initial feedback it was clear that while our architectural foundation was solid, it 
generally lacked clarity and specificity regarding important implementation details. As such, 
we did additional research refining our plans for reliably using emoji actions for bot interaction 
as well as outlining how our key components interact with each other (thankfully very little!). 
After making these changes we hoped that our formal reviewers would have an easier time understanding 
architectural modules so that they could focus on new issues resulting from our revision process.

COMPONENT DESIGN ANALYSES
Building off our initial architecture and refinement, we left the process confident that we had a 
firm grasp of important major components very early on. We also knew that our average technical 
level across the team would ensure that all of our components were implementable after some design. 
Across the three primary modules (rolling, initiative, and db lookup), we had a clear description of 
each component design as well as a straightforward analysis of how they would interact with each other. 
Again, we realize in hindsight that we needed more clarity regarding emoji interaction within initiative 
tracking, as we had described it as an outstanding issue in our prior submission. We also didn’t have a 
formal list of commands for the bot, which left out an important part of design and introduced vagueness 
into the architecture. Thankfully, this was a very straightforward change to make, as we already knew 
informally all necessary commands, so we included those in our next iteration.

CREATING DRAFT ARCHITECTURAL DESCRIPTION
At this point our architecture was well understood by the team, so we were able to convey it firmly in our 
description. Function delivery was clear because we had very little interdependencies between modules, as the 
only module using another was initiative tracking requiring a dice roll for player hierarchy. As we prepared 
for the formal design review, we did our best to then convey this description in a clear and cohesive manner 
such that the other team would be able to identify important flaws in our architecture rather than worry about 
mistakes and vague descriptions.

STUDYING FOR DESIGN REVIEW
Preparing for the formal review process required a significant amount of logistics due to the 12 involved members all 
being scattered across different time zones. Our communication with the immediate group we were reviewing was great. 
We set up a group chat for real-time communication and tried to be as responsive as possible. Each team member did 
their best to take detailed notes on the other team’s architecture and submit them more than 24 hours in advance of 
the scheduled meeting. Our group also met before the formal review to go over our notes and make sure we were consistent 
in our questions as well as ensuring we were critiquing the architecture and not the thoughts of the team members behind 
it. The logistical challenge came from the fact that we had a third review group, as there were an odd number of teams. 
It was difficult to communicate with this group and it was not immediately clear whether they were to help us review, or 
review us. Thankfully, we managed to work out details in advance of the review, and we don’t believe that this was 
necessarily an issue with our process, as this set of interactions was outside of the standard procedure.

THE DESIGN REVIEW MEETING (AS A REVIEWER)
When it came down to the actual design review, we had a very difficult time coordinating with the other team since we were 
both working with members in very different time zones. In addition, we were put in a peculiar situation where we had to 
also coordinate with a third team. Communication began quite late, which we hope to rectify for future design reviews. 
However, once we finally nailed down a time, the review meeting went quite smoothly. All of our members participated and 
raised good questions for the other team to consider, and really tried to understand their architecture. We brought up 
some potential issues with malicious users, for example, which they had not considered. Throughout the process, we were 
respectful and remained in scope. Our team’s scribe also took very detailed notes that later helped us create a solid review 
report, and we later received praise from the team being reviewed saying that our questions helped them better their 
product and final architecture.

THE DESIGN REVIEW MEETING (AS A REVIEWEE)
The design review process was very informative for our team during the feedback and questioning portion, 
as we learned which aspects of our architecture were still unclear. Our team did well fielding questions 
from the review team, due to our understanding of the architectural details, as such, we believe we 
appeared well prepared and knowledgeable about issues raised by the review team. We also had a firm 
grasp of implementation details surrounding the rolling class and the API lookup beyond what was 
listed in our document, as we believed that the very specific details were unnecessary to include 
for the architecture review. However, after being asked specific questions surrounding these 
implementations we were grateful for having done so. The review team was primarily concerned with 
implementation details of the web scraper. We understood this concern as it was by far the most 
technical design of our architecture. Looking back, we should have made it clearer that this was 
a feature intended for development and release after our 1.0 build, and that it was less outlined 
as a consequence. We found other feedback very helpful. Their criticism of our UML diagram was 
entirely valid, as it failed to identify a major data object as well as leaving out important 
classes that we had introduced later in the revision process. 

REVIEW REPORT
After the design review, we began to write our review report. Our team’s scribe took very detailed 
notes during the meeting, allowing us to really work together to find the biggest holes in the other 
team’s architecture and think of possible ways they could fix them. During this process, we divided 
our report into sections: must-fix, should-fix, and comments. This allowed us to split up our issues 
in a way that would be readable. As much as possible, we tried to make our notes actionable, and 
suggested possible ways of improving their architecture to address the identified issues wherever 
possible. Because of this, our review was clear and straight to the point, and addressed the issues 
raised during the actual meeting. While we may have misidentified some of the points as must-fix when 
they should have been should-fix issues, overall we did a good job at solidifying the identified issues 
to easy-to-read bullet points that we hope served the other team well in improving their final architecture.

REVISING OUR ARCHITECTURE
After receiving the review report from Team Dewbed, we got to work on revising our architecture to 
address the major issues they identified. We began by reading through and creating a list of actionable 
items based on the must-fix issues mentioned in their report, assigning team members to each one to ensure 
that all of them were addressed. We also identified the most easily addressable should-fix issues and also 
assigned them to team members. During this period, we worked simultaneously on a Google Doc so that we could 
all see the edits and more easily double check our work. Not only did we work to answer the questions they 
raised in their review report, but also edited some of the existing sections to make our architecture more 
clear overall. This included adding a “Commands” section, which detailed each of the commands we hope to 
implement, and how they work. One issue we ran into during this process, however, was an uncertainty 
surrounding CSV interaction with the Discord API. None of us had thought about it very deeply, and we 
definitely needed to discuss it and do some research regarding how we would implement it. During this 
process, we learned that integrating the two might prove to be more tricky than we initially thought, 
though feasible.

It also became clear that our early architecture was missing, or not delivering well on certain details. 
The review report stated that our first UML diagram was unclear and was more distracting than helpful 
in displaying our architecture, so we revised it to provide more clarity and take into consideration new 
classes introduced during the review meeting itself. Additionally, we left out important implementation 
details that required prototyping, our web scraper was the primary offender. To address this, we created 
a simple prototype and found that it was easier than initially expected, and added it to our final architecture. 
Finally, though minor, we did not explicitly specify what happens when the bot fails to parse a command, 
and we also addressed this in our final architecture. The review report proved to be very helpful in making 
sure that we really understood how our ‘bot’ would be implemented. However, even after revising our 
architecture, we noticed that it was not very readable; we had paragraphs that could have been solidified 
down to bullet pointed specifications to make it easier to understand. In the future, we’ll aim for readability.

PLANNING AND MANAGEMENT
We had good ongoing communication throughout the architecture generation process. Dana created a list of 
actionable items for our team to work on at the end of every status check meeting, and ensured that everyone 
understood their jobs and what needed to be done. This was done through a Discord server our team created at 
the start of this project to help stay organized, and we stayed active on this server with updates and 
clarifications. In the future, having a better meeting schedule for Sunday work-times would be ideal. 
Because our team is working on multiple timezones, it proved difficult to find a time during the week 
to meet up and work simultaneously, which is why having our work on a Google Doc where we could all work 
together at different times was so important. Sunday afternoons proved to be the best time for everyone to 
sit down and work together, which sometimes put us in a time crunch for our deliverables. Now that we know 
that Sunday afternoons tend to be free for most of the team, however, we can plan better in the future.

OVERALL PROJECT AS AN EDUCATIONAL EXERCISE
Overall, this was a good learning experience. Understanding how to do proper research on run-time components, 
using the skills we learned to create prototypes and useful diagrams to answer questions about our architecture, 
and thinking about how to make a technical piece more readable and easy for a reviewer to understand were 
all things we got to work on during this process. In addition, the process of reviewing another team’s 
architecture and being reviewed by another team put us in a position where we had to take constructive criticism.
 We believe we took this criticism well, and did our best to take the suggestions that were made to make 
 our architecture more detailed and easy to understand. We understand that this was a learning process, and 
 while it was frustrating at times to feel so in the dark about the process, we believe we did a good job at handling it overall.
