Team: Discord Dragon

Members: Kevin Ayala, Swamik Lamichhane, Max Mingst, Dana Teves

Project:
    Product Plan: https://github.com/Kayala47/SDEV-KEKW/blob/master/management_1a.txt
    Primary author(s): Kevin Ayala, Swamik Lamichhane, Dana Teves


Creation of the Initial Architecture
Our team worked on the initial architecture for the beginning of this development cycle, and then refined our approach after a formal design review and team discussion and research. Our preliminary architecture was focused on outlining the structure of our bot as well as early prototyping for critical modules including dice rolling and interaction with the Python API provided by Discord. Upon reflection, our preliminary architecture had strong foundations, but we failed to deliver certain implementation details which we then reworked. As for things we believe we did well, we allocated individual team members separate major architecture components so that we were able to work and develop simultaneously, and later receive necessary feedback from other teammates that had “fresh eyes” in the sense that they were not a part of the initial drafting. We also worked quickly on developing important prototypes, and our results demonstrated that our bot was both feasible to implement and that we already had examples of bots using similar toolsets. These prototypes helped us work out details that we had glossed over in our early planning. After receiving initial feedback it was clear that while our architectural foundation was solid, it generally lacked clarity and specificity regarding important implementation details. As such, we did additional research refining our plans for reliably using emoji actions for bot interaction as well as outlining how our key components interact with each other (thankfully very little!). After making these changes we hoped that our formal reviewers would have an easier time understanding architectural modules so that they could focus on new issues resulting from our revision process.
Component Design Analyses
Building off our initial architecture and refinement, we left the process confident that we had a firm grasp of important major components very early on. We also knew that our average technical level across the team would ensure that all of our components were implementable after some design. Across the three primary modules (rolling, initiative, and db lookup), we had a clear description of each component design as well as a straightforward analysis of how they would interact with each other. Again, we realize in hindsight that we needed more clarity regarding emoji interaction within initiative tracking, as we had described it as an outstanding issue in our prior submission. We also didn’t have a formal list of commands for the bot, which left out an important part of design and introduced vagueness into the architecture. Thankfully, this was a very straightforward change to make, as we already knew informally all necessary commands, so we included those in our next iteration.
Creating Draft Architectural Description
At this point our architecture was well understood by the team, so we were able to convey it firmly in our description. Function delivery was clear because we had very little interdependencies between modules, as the only module using another was initiative tracking requiring a dice roll for player hierarchy. As we prepared for the formal design review, we did our best to then convey this description in a clear and cohesive manner such that the other team would be able to identify important flaws in our architecture rather than worry about mistakes and vague descriptions.

Studying for Design Review
Preparing for the formal review process required a significant amount of logistics due to the 12 involved members all being scattered across different time zones. Our communication with the immediate group we were reviewing was great. We set up a group chat for real-time communication and tried to be as responsive as possible. Each team member did their best to take detailed notes on the other team’s architecture and submit them more than 24 hours in advance of the scheduled meeting. Our group also met before the formal review to go over our notes and make sure we were consistent in our questions as well as ensuring we were critiquing the architecture and not the thoughts of the team members behind it. The logistical challenge came from the fact that we had a third review group, as there were an odd number of teams. It was difficult to communicate with this group and it was not immediately clear whether they were to help us review, or review us. Thankfully, we managed to work out details in advance of the review, and we don’t believe that this was necessarily an issue with our process, as this set of interactions was outside of the standard procedure.
The Design Review Meeting (As A Reviewer)
When it came down to the actual design review, we had a very difficult time coordinating with the other team since we were both working with members in very different time zones. In addition, we were put in a peculiar situation where we had to also coordinate with a third team. Communication began quite late, which we hope to rectify for future design reviews. However, once we finally nailed down a time, the review meeting went quite smoothly. All of our members participated and raised good questions for the other team to consider, and really tried to understand their architecture. We brought up some potential issues with malicious users, for example, which they had not considered. Throughout the process, we were respectful and remained in scope. Our team’s scribe also took very detailed notes that later helped us create a solid review report, and we later received praise from the team being reviewed saying that our questions helped them better their product and final architecture.
The Design Review Meeting (As A Reviewee)
The design review process was very informative for our team during the feedback and questioning portion, as we learned which aspects of our architecture were still unclear. Our team did well fielding questions from the review team, due to our understanding of the architectural details, as such, we believe we appeared well prepared and knowledgeable about issues raised by the review team. We also had a firm grasp of implementation details surrounding the rolling class and the API lookup beyond what was listed in our document, as we believed that the very specific details were unnecessary to include for the architecture review. However, after being asked specific questions surrounding these implementations we were grateful for having done so. The review team was primarily concerned with implementation details of the web scraper. We understood this concern as it was by far the most technical design of our architecture. Looking back, we should have made it clearer that this was a feature intended for development and release after our 1.0 build, and that it was less outlined as a consequence. We found other feedback very helpful. Their criticism of our UML diagram was entirely valid, as it failed to identify a major data object as well as leaving out important classes that we had introduced later in the revision process. 
Review Report
After the design review, we began to write our review report. Our team’s scribe took very detailed notes during the meeting, allowing us to really work together to find the biggest holes in the other team’s architecture and think of possible ways they could fix them. During this process, we divided our report into sections: must-fix, should-fix, and comments. This allowed us to split up our issues in a way that would be readable. As much as possible, we tried to make our notes actionable, and suggested possible ways of improving their architecture to address the identified issues wherever possible. Because of this, our review was clear and straight to the point, and addressed the issues raised during the actual meeting. While we may have misidentified some of the points as must-fix when they should have been should-fix issues, overall we did a good job at solidifying the identified issues to easy-to-read bullet points that we hope served the other team well in improving their final architecture.
Revising Our Architecture
After receiving the review report from Team Dewbed, we got to work on revising our architecture to address the major issues they identified. We began by reading through and creating a list of actionable items based on the must-fix issues mentioned in their report, assigning team members to each one to ensure that all of them were addressed. We also identified the most easily addressable should-fix issues and also assigned them to team members. During this period, we worked simultaneously on a Google Doc so that we could all see the edits and more easily double check our work. Not only did we work to answer the questions they raised in their review report, but also edited some of the existing sections to make our architecture more clear overall. This included adding a “Commands” section, which detailed each of the commands we hope to implement, and how they work. One issue we ran into during this process, however, was an uncertainty surrounding CSV interaction with the Discord API. None of us had thought about it very deeply, and we definitely needed to discuss it and do some research regarding how we would implement it. During this process, we learned that integrating the two might prove to be more tricky than we initially thought, though feasible.
It also became clear that our early architecture was missing, or not delivering well on certain details. The review report stated that our first UML diagram was unclear and was more distracting than helpful in displaying our architecture, so we revised it to provide more clarity and take into consideration new classes introduced during the review meeting itself. Additionally, we left out important implementation details that required prototyping, our web scraper was the primary offender. To address this, we created a simple prototype and found that it was easier than initially expected, and added it to our final architecture. Finally, though minor, we did not explicitly specify what happens when the bot fails to parse a command, and we also addressed this in our final architecture. The review report proved to be very helpful in making sure that we really understood how our ‘bot’ would be implemented. However, even after revising our architecture, we noticed that it was not very readable; we had paragraphs that could have been solidified down to bullet pointed specifications to make it easier to understand. In the future, we’ll aim for readability.
Planning and Management
We had good ongoing communication throughout the architecture generation process. Dana created a list of actionable items for our team to work on at the end of every status check meeting, and ensured that everyone understood their jobs and what needed to be done. This was done through a Discord server our team created at the start of this project to help stay organized, and we stayed active on this server with updates and clarifications. In the future, having a better meeting schedule for Sunday work-times would be ideal. Because our team is working on multiple timezones, it proved difficult to find a time during the week to meet up and work simultaneously, which is why having our work on a Google Doc where we could all work together at different times was so important. Sunday afternoons proved to be the best time for everyone to sit down and work together, which sometimes put us in a time crunch for our deliverables. Now that we know that Sunday afternoons tend to be free for most of the team, however, we can plan better in the future.
Overall Project as an Educational Exercise
Overall, this was a good learning experience. Understanding how to do proper research on run-time components, using the skills we learned to create prototypes and useful diagrams to answer questions about our architecture, and thinking about how to make a technical piece more readable and easy for a reviewer to understand were all things we got to work on during this process. In addition, the process of reviewing another team’s architecture and being reviewed by another team put us in a position where we had to take constructive criticism. We believe we took this criticism well, and did our best to take the suggestions that were made to make our architecture more detailed and easy to understand. We understand that this was a learning process, and while it was frustrating at times to feel so in the dark about the process, we believe we did a good job at handling it overall.
